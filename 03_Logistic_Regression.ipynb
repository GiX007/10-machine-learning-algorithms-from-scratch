{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2HrwDoyGKN+RS7PBsivIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX7000/10-machine-learning-algorithms-from-scratch/blob/main/03_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Logistic Regression classifier\n",
        "\n",
        "Instead of predicting just a continuous value, classification tasks most of times require a non-linear approach, in which we try to predict probabilities. A brief introduction to logistic  regression can be found [here](https://www.youtube.com/watch?v=72AHKztZN44), while for deeper exploration, see this [lecture](https://www.youtube.com/watch?v=4u81xU7BIOc). Here, we will see a quick and dirty implementation of a logistic regression classifier."
      ],
      "metadata": {
        "id": "IzpEiD98W-4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dd_0GUuBOaOK"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) and visualize it."
      ],
      "metadata": {
        "id": "jcqMa_oJm3Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "bc = datasets.load_breast_cancer()\n",
        "\n",
        "# retrive X, y\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "# create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# let's see some things about the data\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print(X_train.dtype, X_test.dtype)\n",
        "print(X_train[5])\n",
        "print(y_train[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX5JHJulg9EU",
        "outputId": "27371c89-dae5-4af6-df9b-f9a9dc84488b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(455, 30) (455,) (114, 30) (114,)\n",
            "float64 float64\n",
            "[1.625e+01 1.951e+01 1.098e+02 8.158e+02 1.026e-01 1.893e-01 2.236e-01\n",
            " 9.194e-02 2.151e-01 6.578e-02 3.147e-01 9.857e-01 3.070e+00 3.312e+01\n",
            " 9.197e-03 5.470e-02 8.079e-02 2.215e-02 2.773e-02 6.355e-03 1.739e+01\n",
            " 2.305e+01 1.221e+02 9.397e+02 1.377e-01 4.462e-01 5.897e-01 1.775e-01\n",
            " 3.318e-01 9.136e-02]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create the Linear Regression model."
      ],
      "metadata": {
        "id": "EFVRFDwRg9uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To take the probabilities of a binary classification task, we will use the [sigmoid function](https://www.youtube.com/watch?v=TPqr8t919YM&t=135s). Instead of MSE, the loss function in this case is the [binary cross-entropy loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a), while [gradient descent](https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/) also helps us to minimize it. We use them as you can see in the below cells."
      ],
      "metadata": {
        "id": "5KW01EsoSFhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the sigmoid function\n",
        "def sigmoid(x):\n",
        "  # clip to avoid overflow in exp large values\n",
        "  x = np.clip(x, -500, 500)\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "zl0JjYycOAJa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the linear regression classifier as a class\n",
        "class my_LogisticRegression:\n",
        "\n",
        "  # init function to define what elements(initialized or not) we are going to use inside this class\n",
        "  def __init__(self, lr=0.001, n_iters=1000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "    # set weights and bias to None for now\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "  # fit function for training\n",
        "  def fit(self, X, y):\n",
        "    # get the shape of the X input\n",
        "    n_samples, n_features = X.shape\n",
        "    # initialize weights and bias to 0 in the appropriate shape. Remember: y = X*w + b\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "\n",
        "    for _ in range(self.n_iters):\n",
        "      # forward pass to calculate the y logits\n",
        "      y_logits = np.dot(X, self.weights) + self.bias\n",
        "      # take the probabilities of y logits(just add an activation function to our linear predictions)\n",
        "      y_pred = sigmoid(y_logits)\n",
        "\n",
        "      # calculate gradients of w and b\n",
        "      dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "      db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "      # update weights and bias\n",
        "      self.weights -= self.lr * dw\n",
        "      self.bias -= self.lr * db\n",
        "\n",
        "\n",
        "  # predict function for inference\n",
        "  def predict(self, X):\n",
        "    # forward pass: just take the probabbilities of y logits\n",
        "    y_pred = sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "    # prediction=0 if y_pred<0.5, else prediction=1\n",
        "    y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "    return y_pred_cls"
      ],
      "metadata": {
        "id": "wD5830AXg897"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train, predict and evaluate the model."
      ],
      "metadata": {
        "id": "0VJQOsddoqpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create ana instance of our linear classifier\n",
        "clf_log_reg = my_LogisticRegression()\n",
        "\n",
        "# train the model\n",
        "clf_log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Agrbr3CFg834"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test set\n",
        "predictions = clf_log_reg.predict(X_test)\n",
        "\n",
        "# Print the predictions and the actual labels\n",
        "print(\"Predictions:\", np.array(predictions))\n",
        "print(\"Actual Labels:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjEvZFZ1mCwm",
        "outputId": "af1f45ce-0d2a-472e-a164-cad5e26c0b84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0]\n",
            "Actual Labels: [1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use how good our model is."
      ],
      "metadata": {
        "id": "TvLyFCJhrl25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "  return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "acc = accuracy(y_test, predictions)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEmODCaZmCtp",
        "outputId": "97ba5ff5-4c6b-4dc5-81ba-31df6af0cecc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It could be better. Let's try for a bigger  learning rate."
      ],
      "metadata": {
        "id": "xTpNkKe3tn2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create ana instance of our linear classifier\n",
        "clf_log_reg_2 = my_LogisticRegression(lr=0.01)\n",
        "\n",
        "# train the model\n",
        "clf_log_reg_2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "k07pwb2ftt3I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test set\n",
        "predictions = clf_log_reg_2.predict(X_test)\n",
        "\n",
        "# let's see now the new mse and r2\n",
        "acc_2 = accuracy(y_test, predictions)\n",
        "print(acc_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syW75pKOt5Ud",
        "outputId": "f5f46133-31c7-4845-c216-92c85e344a23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9210526315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For bigger learbing rate, we got better accuracy."
      ],
      "metadata": {
        "id": "WhvPjxqyuNi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Compare with LinearRRegression classifier from scikit-learn library."
      ],
      "metadata": {
        "id": "7QrFGdlqmOX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see now, what results a [LinearRegression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from scikit-learn library gives us."
      ],
      "metadata": {
        "id": "Al8x6nk-uaHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's compare now with the accuracy that sklearn gives us\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_sklearn = LogisticRegression(max_iter=100)\n",
        "clf_sklearn.fit(X_train, y_train)\n",
        "sklearn_predictions = clf_sklearn.predict(X_test)"
      ],
      "metadata": {
        "id": "wsDt0S2EmCqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1495ce86-7dbb-4c84-b71f-1edb065503a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the accuracy now\n",
        "acc_sklearn = accuracy(y_test, sklearn_predictions)\n",
        "print(acc_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYMFqAQ9yvJ1",
        "outputId": "8954c82f-26bf-4df4-995a-4b698e0ef674"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9385964912280702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we get better results from our above custom classifier."
      ],
      "metadata": {
        "id": "oIFMHeDDzmCo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLCzGIu6ntr2"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}